\documentclass[]{article}

\usepackage{amsmath,amssymb,amsfonts}

%opening
\title{HON}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Trajectory}
Here are some thoughts I had about the way we handle trajectories.

\texttt{\$ python3 nHON\textbackslash src\textbackslash midi\_to\_trajectory.py <midi\_corupus\_file> <save\_file>}

\subsection{Single Musical Piece}
In the earlier implementation we split each musical pieces into multiple trajectories separated by \texttt{129}. A potential problem with this is that we are forcing independence between the trajectories in the same musical piece. 

As an alternative, we can keep a musical piece as one trajectory, and \texttt{129} as another ``note''. We can then let HON handle the if there are dependencies are not. This is how it is handled in the script \texttt{midi\_to\_trajectory.py}.

\subsection{Multiple Musical Piece}
In the earlier implementation, different musical pieces are considered as completely separate. When we construct the \texttt{HON}, we construct them separately. This is helpful in understanding the structure of individual musical pieces.

In this approach, to understand the structure of a group of pieces (for example same genre, or same composer), we need to extract features from the multiple \texttt{HON}s and do some form of aggregation.

In the current implementation, different musical pieces are considered as separate trajectories but in the same trajectory file. When we construct the \textit{HON} the different trajectories in the same file are in the same \textit{HON}.

So, we have one \textit{HON} for, say, an genre. If we want to compare different genre we can compare the \textit{HON}s for these genre as simple graph comparison.

\section{Higher Order Network}

Let us represent a higher order rule $u_i \rightarrow u_{i+1}$ with priors $u_{i-l}, u_{i-l+1}, \ldots, u_{i-1}$ with,
\[
u_{i\vert l} \rightarrow u_{i+1}
\]

The \textit{support} of $u_{i\vert l} \rightarrow u_{i+1}$ is the number of times it is observed in the dataset. And it is represented by $\mathcal{S}\left(u_{i\vert l} \rightarrow u_{i+1}\right)$.

The \textit{confidence} of $u_{i\vert l} \rightarrow u_{i+1}$ is the ration,
\[
\mathcal{C}\left(u_{i\vert l} \rightarrow u_{i+1}\right) = \frac{\mathcal{S}\left(u_{i\vert l} \rightarrow u_{i+1}\right)}{\mathcal{S}\left(u_{i\vert l} \rightarrow \ast\right)}
\]
where $\ast$ is any node.

In the current implementation of \texttt{hon.py}, there are two parameters -- \textit{min\_support} ($S_m$) and \textit{delta\_confidence} ($C_\delta$). In the original implementation of \textit{HON}, $S_m$ is the only parameter.

A rule $u_{i\vert l} \rightarrow u_{i+1}$ is considered for inclusion in the \textit{HON} if
\[
\mathcal{S}\left(u_{i\vert l} \rightarrow u_{i+1}\right) \ge S_m
\]

A higher order rule $u_{i\vert l} \rightarrow u_{i+1}$ is included in the \textit{HON} only if the difference in confidence on adding more priors changes the confidence by atleast $C_\delta$. 

Suppose $u_{i\vert l-k} \rightarrow u_{i+1}$ is the longest rule of the form $u_{i\vert \ast} \rightarrow u_{i+1}$ that is already included in the \textit{HON}. A higher order rule $u_{i\vert l} \rightarrow u_{i+1}$ is included if and only if,
\[
\vert\mathcal{C}\left(u_{i\vert l} \rightarrow u_{i+1}\right) - \mathcal{C}\left(u_{i\vert l-k} \rightarrow u_{i+1}\right)\vert \ge C_\delta
\]

The confidence of a rule is not equal to the transition probability for $C_m > 1$. So, the transition probabilities are computed separately based on the support after the \textit{HON} has been created.

Run the script as,

\texttt{\$ python3 nHON\textbackslash src\textbackslash <trajectory\_file> <save\_file>}


\section{Prediction}
Suppose we have multiple \textit{HON}s a trajectory, and we want to find out which \textit{HON} it is most similar to. An example of such task is if we have multiple \textit{HON}s representing musical pieces of different genre, and we have a musical piece that we want to predict the genre of.

In the earlier implementation, we create multiple \textit{HON}s and extract features and approach the problem as a machine learning problem.

In the current approach, we can view the problem as finding the most likely \textit{HON} that generates the trajectory under random walk.

Let us represent the set of \textit{HON}s (also called models) by $M$, and suppose the trajectory is $t = [u_0,u_1,\ldots,u_n]$. 

Represent the transition probability of $u_{i\vert l} \rightarrow u_{i+1}$ in $m \in M$ by $P\left(u_{i\vert l} \rightarrow u_{i+1}, m\right)$.

Represent the rule with the longest prior that matches $u_{i\vert l} \rightarrow u_{i+1}$ in $m$ by $\mathcal{L}\left(u_{i\vert l} \rightarrow u_{i+1}, m\right)$.

Then the log probability that $t$ came from $m$ is given by,
\[
\mathcal{P}\left(t,m\right) = \sum_{i = 0:n} \log\left(P\left(\mathcal{L}\left(u_{i\vert i} \rightarrow u_{n},m\right),m\right)\right)
\]

The predicted model for $t$ is the one with the max log probability.
\[
L\left(t\right) = \underset{m \in M}{\text{argmax }} \mathcal{P}\left(t,m\right)
\]

Run the script with

\texttt{\$ python3 nHON\textbackslash src\textbackslash model\_comparison.py <trajectory\_file\_0> <trajectory\_file\_1> \ldots}


\subsection{Experiment}
Experiment was performed for classification between Classical and Jazz music. The available trajectories are split into test set and training set randomly with ratio $9:1$. The \textit{HON}s for the different genre are created using the test trajectories, and the prediction is done with the training trajectories.

The entire experiment was repeated $30$ times. The confusion matrix is shown in Table \ref{tab:prediction}.

\begin{table}
	\centering
	\begin{tabular}{c|c|c|}
		& Classical (Predicted) & Jazz (Predicted)  \\ 
		\hline 
		Classical (Actual)& 420  & 30  \\ 
		\hline 
		Jazz (Actual)& 121  & 479  \\ 
		\hline 
	\end{tabular} 
	\caption{Results for prediction.} \label{tab:prediction}
\end{table}

(Cannot compare directly with the previous approach using ML because we did not do Classical vs Jazz, and I am too lazy to do it again.)

\section{Time Duration}
One thing we are still missing is the time duration. As discussed, I tried concatenating it with the notes, and adding the durations of consecutive and same notes. That results in networks that are very sparse. So, not sure if thats a good approach.

\section{Centrality}

In most of the cases, when we want the centrality values we want them for the nodes -- not the node and the combination of nodes and priors.

As an example, assume we have the following probabilities,
\begin{align*}
	P \left(a \vert b \rightarrow c\right) &= p_0\\
	P \left(b \vert a \rightarrow c\right) &= p_1\\
	P \left(a \vert c \rightarrow b\right) &= p_2\\
	\vdots
\end{align*}

In most applications, the centrality values we want are that of $a,b,c \ldots$ not $(a\vert b), (b \vert c), \ldots$. So, computing the centrality values is not a simple case of straight forward application of the centrality functions from the existing network libraries such an networkx.

The most obvious way is to the represent a HON of order $m$ and nodes $n$ as a tensor of dimensions $n\times n \times m$. (But I am not sure if we compute stuffs like eigenvalue etc. easily on tensors. Maybe someone who is more familiar can weigh in here.)

\textbf{Alternative Approach}:

Assume that we have a HON of order $m$, and we have a rule
\[
\left(u_i \vert u_{i-1}, \ldots, u_{i-m}\right) \rightarrow u_{i+1}
\]

It can be written as,
\begin{align*}
	%\left(u_i \vert u_{i-1}, \ldots, u_{i-m}\right) &\rightarrow \left(u_{i+1} \vert u_i, u_{i-1}, \ldots, u_{i-m-1}\right)\\
	\left(u_i, u_{i-1}, \ldots, u_{i-m}\right) &\rightarrow \left(u_{i+1}, u_i, u_{i-1}, \ldots, u_{i-m+1}\right)
\end{align*}

This makes it possible to express the probabilities as a two-dimensional matrix where each rows and columns are of the form $(u_i, u_{i-1}, \ldots, u_{i-m})$. Now that we have a matrix we can use the usual methods.

However this introduces two practical problems:
\begin{enumerate}
	\item The size of the matrix will be $n^{m+1} \times n^{m+1}$. This is too big for any real application.
	\item A high-order node generated on the right may not exist as a left hand in the rules generated. This is because the rule generation process does not accept all possible rules. As a result of this, it will introduce \textit{false} $0$ probabilities.
\end{enumerate}

We can solve both these problems in the following ways:

Let $\mathbb{R}$ be the set of all the rules generated. A rule that HON generates is of the form 
\begin{align*}
	r &= \left(u_i, \ldots, u_{i-m}\right) \rightarrow \left(u_{i+1}, \ldots, u_{i-m+1}\right) \\
	& = r_L \rightarrow r_R	
\end{align*}

Then let us define the following sets,
\begin{align*}
	\mathbb{R}_L & = \left\{ r_L : \forall r \in \mathbb{R} \right\}\\
	\mathbb{R}_R & = \left\{ r_R : \forall r \in \mathbb{R} \right\}
\end{align*}

Define a function $\mathcal{L}: \mathbb{R}_R \times \mathbb{R}_L \rightarrow \mathbb{Z}$ that matches (consecutively) an element from $\mathbb{R}_R$ to another from $\mathbb{R}_L$ from the left and outputs the length of common match.

Define a function $\mathcal{M}:\mathbb{R}_R \rightarrow \mathbb{R}_L$ such that
\begin{align*}
\mathcal{M}\left(r_R\right) = \underset{r_L \in \mathbb{R}_L}{\text{argmax }} \mathcal{L}\left(r_R, r_L\right)
\end{align*}

If there are sink nodes, it is possible to have the case $\mathcal{M}\left(r_R\right) = \phi$. To handle these cases,
\begin{align*}
	\mathcal{M}^\prime\left(r_R\right) = \begin{cases}
	\mathcal{M}\left(r_R\right) & \text{if } \mathcal{M}\left(r_R\right) \ne \phi \\
	r_R[0] & \text{if } \mathcal{M}\left(r_R\right) = \phi
	\end{cases}
\end{align*}

Then we can convert every rule to the form,
\[
r = r_L \rightarrow \mathcal{M}\left(r_R\right)
\]

Then, the size of the generated matrix will be approximately of the size $\vert \mathbb{R}_L \vert ^ 2$.
Since this is much smaller than $n^{2(m+1)}$ and all the $r_R$ are mapped to some $r_L$, it takes care of both the problems mentioned.

Let us represent the matrix generated by $A$.

\subsection{PageRank}

Let $I$ be the image of $\mathcal{M}^\prime\left(r_R\right), \forall r_R \in \mathbb{R}_R$.

\[
PR\left(u\right) = \sum_{r \in I} PR\left(r\right) \cdot \delta\left(u,r\right)
\]

\[
\delta\left(u,r\right) = \begin{cases}
1 & \text{if } r = r[0]\\
0 & \text{otherwise}
\end{cases}
\]

\subsection{Closeness Centrality}
For $x,y \in I$, let $\rho(x,y)$ be the shortest path between $x$ and $y$.

For $u,v \in V$, let us define the length of the shortest path as $\xi\left(u,v\right)$ as
\[
\xi\left(u\right) = \underset{\substack{x,y \in I\\ x[0]=u\\y[0]=v}}{\text{min }}\left\vert \rho\left(x,y\right)\right\vert
\]

For $u \in V$, the closeness centrality is
\[
C(u) = \sum_{v \in V} \frac{1}{\xi\left(u,v\right)}
\]

\subsection{Betweeness Centrality}

Let $\iota(u,v)$ be the number of shortest paths between all $x,y \in I$ where $x[0] = u, y[0]=v$. 

Let $\iota^\prime\left(u,v,w\right)$ be the number of shortest paths between all $x,y \in I$, where $x[0] = u, y[0] = v$ that passes thorough $z \in I$, where $z[0] = w$.

Then the betweeness centrality is,

\[
B\left(u\right) = \sum_{\substack{s,t \in V\setminus\left\{u\right\}\\s \ne t}}\frac{\iota^\prime\left(s,t,u\right)}{\iota\left(s,t\right)}
\]


\end{document}
